## 🗓️ Day 1 – Introduction to LLMs

### ✅ What I Learned

- **LLM stands for**: *Large Language Model*.
- These are **AI models trained on massive text datasets** to understand and generate human-like text.
- **Examples**:
  - 🔹 ChatGPT (powered by OpenAI’s GPT-4o)
  - 🔹 Claude (by Anthropic)
- **Trained on**: Billions of tokens (words, subwords, or characters).
- **Model size**: LLMs have billions of parameters that define how they process and generate language.
- **Core architecture**: 🔧 **Transformer**
  - Introduced in the paper *"Attention is All You Need"*
- **Tokenization**: Input text is split into smaller units called *tokens*, which the model processes.
- **Reinforcement Learning**:
  - Used during the fine-tuning stage, often via **Reinforcement Learning from Human Feedback (RLHF)** to improve the quality of responses.

---

## 🔍 Concepts I Now Understand

| Concept               | Description                                                                 |
|-----------------------|-----------------------------------------------------------------------------|
| LLM                   | AI model that generates human-like text                                     |
| Transformer           | Model architecture using attention to understand context                    |
| Tokenization          | Splitting input text into smaller units (tokens) for processing             |
| (Reinforcement)  | Fine-tuning technique using human feedback                                       |
